{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas funciton API\n",
    "Spark有一些函数可以让python的函数通过pandas实例直接用在spark dataframe上。内部机制上类似`pandas udf`, jvm把数据转成arrow的buffer, 然后pandas可以直接在buffer上操作。但是区别是，这些函数api使用起来就像普通pyspark api一样是作用在dataframe上的, 而不像udf那样作用于一个`column`. 实际使用的时候，一般写法是`DataFrame.groupby().applyInPandas()`或者`DataFrame.groupby().mapInPandas()`\n",
    "\n",
    "### Grouped map api\n",
    "Spark的dataframe在`groupby`后使用普通的pandas函数， 如`df.groupby().applyInPandas(func, schema))`， 普通的pandas函数需要输入是pandas dataframe, 返回普通的pandas dataframe. 上面这写法会把每个分组group映射到pandas dataframe.\n",
    "`df.groupby().applyInPandas(func, schema))`过程其实分为三步， 典型的`split-apply-combine`模式：\n",
    "- `DataFrame.groupBy`分组数据\n",
    "- 分组的数据映射到pandas dataframe后，apply到传入的函数\n",
    "- 组合结果成一个新的pyspark Dataframe\n",
    "\n",
    "使用groupBy().applyInPandas(), 用户需要做两件事：\n",
    "- 写好pandas函数,输入dataframe, 输出dataframe\n",
    "- 定义好pyspark dataframe结果的schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|                 v|\n",
      "+---+------------------+\n",
      "|  1|              -0.5|\n",
      "|  1|               0.5|\n",
      "|  2|-2.666666666666667|\n",
      "|  2|-1.666666666666667|\n",
      "|  2| 4.333333333333333|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"app1\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([(1,1.0), (1,2.0),(2,3.0),(2,4.0),(2,10.0)], ('id','v'))\n",
    "def substract_mean(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v-v.mean())\n",
    "\n",
    "df.groupby('id').applyInPandas(substract_mean, schema='id long, v double').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map api\n",
    "也可以对pyspark dataframe和pandas dataframe做map操作，`DataFrame.mapInPandas()`是对当前的DataFrame的取一个迭代器映射普通到pandas函数。这个普通pandas函数必须是输入输出都是pdf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|  v|\n",
      "+---+---+\n",
      "|  1|1.0|\n",
      "|  1|2.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_func(iterator):\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id==1]\n",
    "        \n",
    "df.mapInPandas(filter_func, schema=df.schema).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### co-grouped map api\n",
    "这个api可以使两个pyspark dataframe组合后使用pandas函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+\n",
      "|    time| id| v1|\n",
      "+--------+---+---+\n",
      "|20000101|  1|1.0|\n",
      "|20000101|  2|2.0|\n",
      "|20000102|  1|3.0|\n",
      "|20000102|  2|4.0|\n",
      "+--------+---+---+\n",
      "\n",
      "+--------+---+---+\n",
      "|    time| id| v2|\n",
      "+--------+---+---+\n",
      "|20000101|  1|  x|\n",
      "|20000101|  2|  y|\n",
      "+--------+---+---+\n",
      "\n",
      "+--------+---+---+---+\n",
      "|    time| id| v1| v2|\n",
      "+--------+---+---+---+\n",
      "|20000101|  1|1.0|  x|\n",
      "|20000102|  1|3.0|  x|\n",
      "|20000101|  2|2.0|  y|\n",
      "|20000102|  2|4.0|  y|\n",
      "+--------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
    "    (\"time\", \"id\", \"v2\"))\n",
    "df1.show()\n",
    "df2.show()\n",
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
    "\n",
    "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
    "    asof_join, schema=\"time int, id int, v1 double, v2 string\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
